# ğŸ›¡ï¸ MÃ³dulo 6: Ã‰tica y privacidad

El uso de herramientas de inteligencia artificial plantea importantes cuestiones Ã©ticas y de protecciÃ³n de datos.  
Este mÃ³dulo te ayudarÃ¡ a comprender los **riesgos**, las **responsabilidades** y las **buenas prÃ¡cticas** relacionadas con la privacidad y el uso Ã©tico de la IA en el entorno laboral.

---

## Riesgos de privacidad y confidencialidad al usar IA

Las IA como ChatGPT procesan texto, pero no tienen memoria privada dentro de la empresa.  
Esto significa que:

- No debes compartir **datos sensibles**: nombres completos de clientes, DNI, cuentas bancarias, contraseÃ±as, etc.
- Todo lo que se introduce en una IA pÃºblica puede ser almacenado y analizado por el proveedor, aunque sea de forma anÃ³nima.
- Algunas plataformas permiten entrenar modelos internos, pero requieren configuraciones especÃ­ficas de seguridad.

**Ejemplo de mal uso:**
> "ChatGPT, redacta este contrato para el cliente MarÃ­a LÃ³pez con NIF 98765432X y direcciÃ³n Calle Mayor 25..."

---

## CÃ³mo proteger informaciÃ³n sensible y cumplir normativas

1. **Evita introducir datos personales o confidenciales** en herramientas abiertas.
2. **Consulta con IT o Legal** si necesitas usar IA para datos reales de la empresa.
3. **Utiliza IA local o privada** cuando sea posible (por ejemplo, Microsoft Copilot bajo polÃ­ticas internas).
4. **Avisa siempre** si un contenido ha sido generado con IA (transparencia).
5. **Cumple el RGPD y otras normativas** segÃºn tu rol y los datos que manejes.

### Herramientas con enfoque seguro:
- Microsoft Copilot (M365)
- ChatGPT Team / Enterprise
- Plataformas con control de datos (locales o cloud privados)

---

## LÃ­mites Ã©ticos del uso de la IA en contextos laborales

MÃ¡s allÃ¡ de lo legal, hay lÃ­mites que conviene respetar en cuanto a:

- **RepresentaciÃ³n honesta del trabajo**: no presentar como propio algo 100% generado por IA sin revisiÃ³n.
- **Evitar sesgos**: la IA puede reproducir estereotipos o discriminaciones si no se revisa.
- **No depender ciegamente de la tecnologÃ­a**: siempre debe haber supervisiÃ³n humana.
- **Uso transparente**: si colaboras con IA, dilo cuando sea relevante.

**Ejemplo positivo:**
> "Este informe ha sido generado con la ayuda de una herramienta de IA y revisado por el equipo de contabilidad."

---

## Actividad prÃ¡ctica

> Reflexiona sobre cÃ³mo usas actualmente herramientas de IA:  
> - Â¿Has introducido alguna vez informaciÃ³n sensible sin darte cuenta?  
> - Â¿CuÃ¡ndo consideras que serÃ­a poco Ã©tico usar una IA?  
> - Â¿CÃ³mo puedes mejorar tu uso responsable a partir de ahora?

Escribe tus respuestas y compÃ¡rtelas en grupo si lo deseas.

---

## Recursos adicionales

- [GuÃ­a de privacidad y seguridad en IA (Agencia EspaÃ±ola de ProtecciÃ³n de Datos)](https://www.aepd.es/)
- [Principios Ã©ticos para la IA (ComisiÃ³n Europea)](https://digital-strategy.ec.europa.eu/)
- [Normativa sobre uso de IA en el entorno laboral (PDF interno)](/oficina_basico/stuff/etica_privacidad_navima.pdf)

---

<p align="center">
  <a href="https://hugocnl11.github.io/Formacion-interna-Navima/oficina_basico/modulo_5.html">â®ï¸ MÃ³dulo anterior</a> 
  <a href="https://hugocnl11.github.io/Formacion-interna-Navima/curso_ia_oficina.html">DescripciÃ³n del curso ğŸ“š</a>
</p>
